# Evaluate Your Own Tasks

## YAML file for tasks

We use the YAML file to define tasks, this allows us to easily evaluate multiple tasks at a single run and configure them independently. Specifically, you can add multiple tasks or folders  at a time for evaluation, and the script will automatically collect all YAML files under those folders recursively.

```
bash scripts/evaluate.sh ChatGPT task1.yaml task2.yaml dir1 dir2 ...
```
First argument will be the model name, we support ChatGPT and ChatGLM currently.
After that you can add your YAML files.
We support generation evaluation tasks. Basically, all types of tasks share common configs defining task information:

```yaml
name: 'glue_cola'  # Task Name
type: 'gen'  # Task type, 'gen' (generate)
path: 'bloom/glue_cola'  # task data path relative to DATA_PATH in 'evaluate.sh'
file-pattern: # Organize jsonl file in groups
  validation: "**/validation.jsonl" # Will search for all file named 'validation.jsonl' in 
workers: 30 # Parallel Call Limit
shot: 0 # (zero-shot or few-shot, Set to 0 by default.
language: "en" # language of your dataset, "en" or "cn".Set to "en" by default.
```

See configuration details for generation task in `evaluation/configs.py`.

## Data format for tasks

We recommend organizing the task data in the following structure and setup up two groups named "validation" and "test" in the `file-pattern` config so that it becomes very easy to evaluate different prompts on both validation and test sets independently.

```bash
DATA_PATH
└── task_name
    ├── prompt_1
    │   ├── test.jsonl
    │   └── val.jsonl
    ├── prompt_2
    │   ├── test.jsonl
    │   └── val.jsonl
    └── prompt_3
        ├── test.jsonl
        └── val.jsonl
```

The evaluation data for each prompt are organized into jsonline format.

For the generation task, the format of each line of JSON should be

```json
{
    "inputs_pretokenized": "Context and question here",
    "targets_pretokenized": ["Target 1", "Target 2", "Target 3"],
    "label": int
}
```

The default metrics for the generation task are BLEU and ROUGE-1,2,L. Given inputs, the sequence generated by the model will be metricized separately from all targets and the highest value will be taken.


## Implement Your Metrics

You can customize your evaluation metrics function and add it to `DEFAULT_METRICS` in `evaluation/metrics.py`, and then you can specify `metric: ['Your metric name']` in the task YAML file.

## Fully customize the evaluation process

By default, we implement classes named  `GenerationTask` in `evaluation/tasks.py` for generation tasks.

You can implement a new task class and inherit from it.

You can add new metrics to your task, see `tasks/test/example.yaml` .

Once you have created the new task class, you need to specify the relative path to import the class in the `module` field of the task YAML file.  